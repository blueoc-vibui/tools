import os
import time
import requests
import concurrent.futures
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import random

# Nháº­p thÃ´ng tin tá»« ngÆ°á»i dÃ¹ng
base_url = "https://www.teepublic.com"
path = input("Nháº­p path (vÃ­ dá»¥: /t-shirts): ").strip()
start_page = int(input("Nháº­p trang báº¯t Ä‘áº§u: "))
end_page = int(input("Nháº­p trang káº¿t thÃºc: "))

# Cáº¥u hÃ¬nh proxy
proxy = {
    "http": "http://root:kPEV5b4r@23.129.232.126:56259",
    "https": "http://root:kPEV5b4r@23.129.232.126:56259"
}

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

# Cáº¥u hÃ¬nh session vá»›i Retry
session = requests.Session()
retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))

# Táº¡o thÆ° má»¥c lÆ°u áº£nh
save_folder = "teepublic"
os.makedirs(save_folder, exist_ok=True)

def get_product_links(page_number):
    """Láº¥y danh sÃ¡ch href tá»« má»™t trang cá»¥ thá»ƒ."""
    url = f"{base_url}{path}{'&' if '?' in path else '?'}page={page}"
    print(url)
    time.sleep(2)  # Chá» Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
    response = session.get(url, headers=headers, proxies=proxy, timeout=10)
    if response.status_code != 200:
        print(f"KhÃ´ng thá»ƒ truy cáº­p trang {url}")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.select(".tp-design-tile__image-wrap > a:first-child")
    return [link.get("href") for link in links if link.get("href")]

def download_image(idx, href):
    """Táº£i áº£nh tá»« trang sáº£n pháº©m."""
    try:
        time.sleep(2)  # TrÃ¡nh bá»‹ cháº·n do gá»­i request quÃ¡ nhanh
        product_page = session.get(base_url + href, headers=headers, proxies=proxy, timeout=10)
        if product_page.status_code != 200:
            print(f"âŒ KhÃ´ng thá»ƒ truy cáº­p {href}")
            return

        product_soup = BeautifulSoup(product_page.text, "html.parser")
        img_tag = product_soup.select_one("img.jsProductMainImage")

        if img_tag and img_tag.get("src"):
            img_url = img_tag["src"]
            img_name = img_tag["alt"].replace("/", "-")  # TrÃ¡nh lá»—i tÃªn file
            img_path = os.path.join(save_folder, f"{img_name}.jpg")

            # Táº£i áº£nh vá» qua proxy
            response = session.get(img_url, stream=True, proxies=proxy, timeout=10)
            if response.status_code == 200:
                with open(img_path, "wb") as file:
                    for chunk in response.iter_content(1024):
                        file.write(chunk)
                print(f"âœ… áº¢nh {idx+1} táº£i vá»: {img_name}")
            else:
                print(f"âŒ Lá»—i táº£i áº£nh {idx+1}: {img_url}")
        else:
            print(f"âš  KhÃ´ng tÃ¬m tháº¥y áº£nh chÃ­nh trÃªn trang {href}")
    except Exception as e:
        print(f"âŒ Lá»—i khi xá»­ lÃ½ {href}: {e}")

# Láº¥y danh sÃ¡ch sáº£n pháº©m tá»« nhiá»u trang
all_hrefs = []
for page in range(start_page, end_page + 1):
    print(f"ğŸ” Äang láº¥y danh sÃ¡ch sáº£n pháº©m tá»« trang {page}...")
    all_hrefs.extend(get_product_links(page))

print(f"Tá»•ng sá»‘ sáº£n pháº©m tÃ¬m tháº¥y: {len(all_hrefs)}. Báº¯t Ä‘áº§u táº£i áº£nh...")

# Sá»­ dá»¥ng ThreadPoolExecutor Ä‘á»ƒ giá»›i háº¡n sá»‘ luá»“ng (giáº£m lá»—i káº¿t ná»‘i)
max_threads = 5
with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
    executor.map(download_image, range(len(all_hrefs)), all_hrefs)

print("âœ… HoÃ n thÃ nh táº£i áº£nh!")