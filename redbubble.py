import os
import time
import cloudscraper
import concurrent.futures
from bs4 import BeautifulSoup
import random

# Sá»­ dá»¥ng cloudscraper Ä‘á»ƒ vÆ°á»£t Cloudflare
session = cloudscraper.create_scraper()

# Nháº­p thÃ´ng tin tá»« ngÆ°á»i dÃ¹ng
base_url = "https://www.redbubble.com"
path = input("Nháº­p path (vÃ­ dá»¥: /t-shirts): ").strip()
start_page = int(input("Nháº­p trang báº¯t Ä‘áº§u: "))
end_page = int(input("Nháº­p trang káº¿t thÃºc: "))

proxy = {
    "http": "http://root:kPEV5b4r@23.129.232.126:56259",
    "https": "http://root:kPEV5b4r@23.129.232.126:56259"
}

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
}

# Táº¡o thÆ° má»¥c lÆ°u áº£nh
save_folder = "redbubble"
os.makedirs(save_folder, exist_ok=True)

import re
def clean_filename(filename, max_length=255):
    """Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»›i háº¡n Ä‘á»™ dÃ i tÃªn file."""
    filename = re.sub(r'[^a-zA-Z0-9 _-]', '', filename)  # Chá»‰ giá»¯ chá»¯, sá»‘, `_` vÃ  `-`
    return filename.strip().replace(' ', '_').replace('_', ' ')[:max_length]

def get_product_links(page_number):
    """Láº¥y danh sÃ¡ch href tá»« má»™t trang cá»¥ thá»ƒ."""
    url = f"{base_url}{path}{'&' if '?' in path else '?'}page={page}"
    time.sleep(4)  # TrÃ¡nh bá»‹ cháº·n
    response = session.get(url, headers=headers, proxies=proxy, timeout=10)
    if response.status_code != 200:
        print(f"KhÃ´ng thá»ƒ truy cáº­p trang {url}")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.select("a[data-testid='related-work-card']")
    return [link.get("href") for link in links if link.get("href")]

def download_image(idx, href):
    """Táº£i áº£nh tá»« trang sáº£n pháº©m."""
    try:
        time.sleep(random.uniform(2, 5))  # TrÃ¡nh bá»‹ cháº·n do gá»­i request quÃ¡ nhanh
        product_page = session.get(base_url + href, headers=headers, proxies=proxy, timeout=10)
        if product_page.status_code != 200:
            print(f"âŒ KhÃ´ng thá»ƒ truy cáº­p {href}")
            return

        # LÆ°u Cookie tá»« trang sáº£n pháº©m
        session.cookies.update(product_page.cookies)

        product_soup = BeautifulSoup(product_page.text, "html.parser")
        img_tag = product_soup.select("picture.Picture_picture__Gztgz.Picture_rounded__PvnLg > img")[1]
        
        if not img_tag:
            print(f"âš  KhÃ´ng tÃ¬m tháº¥y áº£nh chÃ­nh trÃªn trang {href}")
            return

        img_url = img_tag["src"]
        img_name = clean_filename(img_tag["alt"])  # TrÃ¡nh lá»—i tÃªn file
        img_path = os.path.join(save_folder, f"{img_name}.jpg")

        # Cáº­p nháº­t headers vá»›i Referer tá»« trang sáº£n pháº©m
        img_headers = headers.copy()
        img_headers["Referer"] = base_url + href  

        # Táº£i áº£nh vá»›i Cookie tá»« session
        response = session.get(img_url, headers=img_headers, proxies=proxy, timeout=10, stream=True)
        if response.status_code == 200:
            with open(img_path, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)
            print(f"âœ… áº¢nh {idx+1} táº£i vá»: {img_name}")
        else:
            print(f"âŒ Lá»—i táº£i áº£nh {idx+1}: {img_url} (Lá»—i {response.status_code})")
    except Exception as e:
        print(f"âŒ Lá»—i khi xá»­ lÃ½ {href}: {e}")

# Láº¥y danh sÃ¡ch sáº£n pháº©m tá»« nhiá»u trang
all_hrefs = []
for page in range(start_page, end_page + 1):
    print(f"ğŸ” Äang láº¥y danh sÃ¡ch sáº£n pháº©m tá»« trang {page}...")
    all_hrefs.extend(get_product_links(page))

print(f"Tá»•ng sá»‘ sáº£n pháº©m tÃ¬m tháº¥y: {len(all_hrefs)}. Báº¯t Ä‘áº§u táº£i áº£nh...")

# Sá»­ dá»¥ng ThreadPoolExecutor Ä‘á»ƒ giá»›i háº¡n sá»‘ luá»“ng (giáº£m lá»—i káº¿t ná»‘i)
max_threads = 5
with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
    executor.map(download_image, range(len(all_hrefs)), all_hrefs)

print("âœ… HoÃ n thÃ nh táº£i áº£nh!")
